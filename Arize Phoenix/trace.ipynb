{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a63a68e0-437d-416a-8993-054cfedb679a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b47602-7580-4ae1-8b94-938a5660b413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install arize-phoenix langchain-core langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5b010d-8e6e-486a-bacb-85bbc31fc99f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import langchain\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9dcdfd4-0ab9-4572-a420-5669ef184e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "open_api_key=\"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82c58f4c-e5ff-4bea-b09e-63a0ff332117",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Launch phoenix\n",
    "px.close_app()\n",
    "session = px.launch_app()\n",
    "\n",
    "# Once you have started a Phoenix server, you can start your LangChain application with the OpenInferenceTracer as a callback. To do this, you will have to instrument your LangChain application with the tracer:\n",
    "\n",
    "from phoenix.trace.langchain import OpenInferenceTracer, LangChainInstrumentor\n",
    "\n",
    "# By default, the traces will be exported to the locally running Phoenix server.\n",
    "LangChainInstrumentor().instrument()\n",
    "\n",
    "# Initialize your LangChain application\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import KNNRetriever\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "documents_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/langchain-pinecone/database.parquet\"\n",
    ")\n",
    "knn_retriever = KNNRetriever(\n",
    "    index=np.stack(documents_df[\"text_vector\"]),\n",
    "    texts=documents_df[\"text\"].tolist(),\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    ")\n",
    "chain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\n",
    "chat_model_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=chat_model_name)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=knn_retriever,\n",
    ")\n",
    "\n",
    "# Instrument the execution of the runs with the tracer. By default the tracer uses an HTTPExporter\n",
    "query = \"What is euclidean distance?\"\n",
    "response = chain.run(query)\n",
    "\n",
    "# By adding the tracer to the callbacks of LangChain, we've created a one-way data connection between your LLM application and Phoenix.\n",
    "\n",
    "# To view the traces in Phoenix, simply open the UI in your browser.\n",
    "session.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eec12a33-7d6c-49d0-877c-81e3bb1d7710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"what is a large language model?\"\n",
    "response = chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0589c1f7-2929-4983-ae51-0cfd0675176c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"How can I improve retrieval performance for my RAG application?\"\n",
    "response = chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69594352-1750-4db6-84e2-ec9a09adb981",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "px.close_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bde56ff4-60d1-40b3-b69c-630a49683d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "trace",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
