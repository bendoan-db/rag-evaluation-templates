{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca76c2d-d755-4426-b170-4fb3529b05a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note: This currently only works locally! Integration with dbtunnel is needed to expose the phoenix UI on a databricks cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7735ab0d-eda6-4b48-b1e0-84cd77336f42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install arize-phoenix[experimental]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cbd151f-8d46-4f6f-8f60-454d450f52d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from phoenix.trace.trace_dataset import TraceDataset\n",
    "from phoenix.trace.utils import json_lines_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deb0e066-0c89-478d-9745-34d6bade27e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = './data/trace.jsonl'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "\n",
    "trace_ds = TraceDataset(json_lines_to_df(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b04ec9b-f638-4d52-bda7-c3ab480f5588",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "session = px.launch_app(trace=trace_ds)\n",
    "#session.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7207ec9-6abb-43da-bb06-e2ffc3054cc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())\n",
    "queries_df = get_qa_with_reference(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5857793a-0d48-47a2-93f9-f73f9854e64f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "retrieved_documents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6407d605-0e2d-46df-ae50-0caa236fd15a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import OpenAIModel\n",
    "import os\n",
    "open_api_key=\"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_api_key\n",
    "\n",
    "eval_model = OpenAIModel(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3323bf24-c9d8-4de3-b8da-958ad21dbbbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import (\n",
    "    HallucinationEvaluator,\n",
    "    QAEvaluator,\n",
    "    RelevanceEvaluator,\n",
    ")\n",
    "\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24a36a3-1d89-4358-aa97-5d504f3e3261",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from phoenix.experimental.evals import (\n",
    "    run_evals,\n",
    ")\n",
    "\n",
    "#nest_asyncio.apply()  # needed for concurrency in notebook environments\n",
    "\n",
    "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
    "    dataframe=queries_df,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d64d86b-8681-4ae3-b323-2c121b94ab6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    "    DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5553a4ec-f1f0-47eb-8588-cbd7820ce32f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"üî•üê¶ Open back up Phoenix in case you closed it: {session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da2c7c99-e507-4d16-8292-2770fe9c2219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get traces from Phoenix into dataframe \n",
    "\n",
    "spans_df = px.active_session().get_spans_dataframe()\n",
    "spans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n",
    "\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.active_session())\n",
    "queries_df = get_qa_with_reference(px.active_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8019e8f3-d61b-4316-9950-ede4cfc22afe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "retrieved_documents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e11338-3f8f-4415-9f11-60b832292f34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98f590ba-fdf8-4c44-bea8-c780561ddff0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations, DocumentEvaluations\n",
    "from phoenix.experimental.evals import (\n",
    "  HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "  HALLUCINATION_PROMPT_TEMPLATE,\n",
    "  QA_PROMPT_RAILS_MAP,\n",
    "  QA_PROMPT_TEMPLATE,\n",
    "  OpenAIModel,\n",
    "  llm_classify,\n",
    ")\n",
    "\n",
    "# Creating Hallucination Eval which checks if the application hallucinated\n",
    "hallucination_eval = llm_classify(\n",
    "  dataframe=queries_df,\n",
    "  model=OpenAIModel(\"gpt-4-turbo-preview\", temperature=0.0),\n",
    "  template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "  rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "  provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "  concurrency=4,\n",
    ")\n",
    "hallucination_eval[\"score\"] = (\n",
    "  hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n",
    ").astype(int)\n",
    "\n",
    "# Creating Q&A Eval which checks if the application answered the question correctly\n",
    "qa_correctness_eval = llm_classify(\n",
    "  dataframe=queries_df,\n",
    "  model=OpenAIModel(\"gpt-4-turbo-preview\", temperature=0.0),\n",
    "  template=QA_PROMPT_TEMPLATE,\n",
    "  rails=list(QA_PROMPT_RAILS_MAP.values()),\n",
    "  provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "  concurrency=4,\n",
    ")\n",
    "\n",
    "qa_correctness_eval[\"score\"] = (\n",
    "  hallucination_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n",
    ").astype(int)\n",
    "\n",
    "# Logs the Evaluations back to the Phoenix User Interface (Optional)\n",
    "px.Client().log_evaluations(\n",
    "  SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n",
    "  SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8004a4a8-3221-47f6-b061-1ae9281d7dba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "retrieved_documents_eval = llm_classify(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    model=OpenAIModel(\"gpt-4-turbo-preview\", temperature=0.0),\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    ")\n",
    "\n",
    "retrieved_documents_eval[\"score\"] = (\n",
    "    retrieved_documents_eval.label[~retrieved_documents_eval.label.isna()] == \"relevant\"\n",
    ").astype(int)\n",
    "\n",
    "px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=retrieved_documents_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09622718-b200-4c09-9528-91cb56470b92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "evaluation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
